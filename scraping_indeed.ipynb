{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Web scraping to indeed.com using the Python library Selenium.**\n",
        "\n",
        "---\n",
        "\n",
        "üìé README\n",
        "\n",
        "This script executes code to scrape googletrends.com. It is necessary to input only the search terms and after save a CSV file with the name scraping_indeed.csv."
      ],
      "metadata": {
        "id": "UIKBSqwmp8n_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-2nDw0aXo4uB"
      },
      "outputs": [],
      "source": [
        "# @title 1. ‚ú® Installing dependences for Selenium.\n",
        "\n",
        "!pip3 install webdriver-manager &> /dev/null\n",
        "!pip3 install selenium &> /dev/null\n",
        "!pip install -U selenium &> /dev/null\n",
        "!pip3 install pandas &> /dev/null\n",
        "!pip3 install aiohttp &> /dev/null\n",
        "!pip3 install asyncio &> /dev/null\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import threading\n",
        "import pandas as pd\n",
        "import selenium\n",
        "import time\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "#from selenium.webdriver.support.ui import WebDriverWait\n",
        "#from selenium.webdriver.support import expected_conditions as EC\n",
        "#from selenium.webdriver.common.keys import Keys\n",
        "#from concurrent.futures import ThreadPoolExecutor, wait"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. ‚úç Run the script to scrape indeed.com.\n",
        "\n",
        "job = \"Data+Engineer\" #@param {type:\"string\"}\n",
        "location = \"Washington\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Finding location, position, radius=35 miles, sort by date and starting page\n",
        "url = 'https://www.indeed.com/jobs?q={}&l={}&radius=35&filter=0&sort=date&start={}'\n",
        "\n",
        "def web_driver():\n",
        "    option= webdriver.ChromeOptions()\n",
        "    option.add_argument(\"--incognito\")\n",
        "    # option.add_argument('--headless=chrome') # This is for to hidden windows\n",
        "    # options.add_argument(\"--verbose\")\n",
        "    # options.add_argument('--no-sandbox')\n",
        "    # options.add_argument('--headless')\n",
        "    # options.add_argument('--disable-gpu')\n",
        "    # options.add_argument(\"--window-size=1920, 1200\")\n",
        "    # options.add_argument('--disable-dev-shm-usage')\n",
        "    # driver = webdriver.Chrome(options=options)\n",
        "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=option)\n",
        "    return driver\n",
        "\n",
        "job_lst=[]\n",
        "\n",
        "def scraping_indeed(url,job,location):\n",
        "\n",
        "    driver = web_driver()\n",
        "\n",
        "    driver.get(url.format(job,location,0))\n",
        "\n",
        "    sleep(randint(2, 6))\n",
        "\n",
        "    # Pages for scraping\n",
        "    pages=driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
        "\n",
        "    # Maxim iteractions for page.\n",
        "    max_iter_pgs=int(pages.split(' ')[0])//15\n",
        "\n",
        "    job_description_list_href=[]\n",
        "    salary_list=[]\n",
        "\n",
        "    for i in range(0,max_iter_pgs):\n",
        "        driver.get(url.format(job_,location,i*10))\n",
        "\n",
        "        sleep(randint(4, 6))\n",
        "\n",
        "        job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
        "        jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
        "\n",
        "        for jj in jobs:\n",
        "            job_title = jj.find_element(By.CLASS_NAME,\"jcs-JobTitle\")\n",
        "\n",
        "            # Link of job.\n",
        "            # ID job\n",
        "            # Company name\n",
        "            # Location\n",
        "            # Posting date\n",
        "\n",
        "            job_lst.append([job_title.text,\n",
        "            job_title.get_attribute(\"href\"),\n",
        "            job_title.get_attribute(\"id\"),\n",
        "            jj.find_element(By.CLASS_NAME,\"css-1x7z1ps\").text,\n",
        "            jj.find_element(By.CLASS_NAME,\"css-t4u72d\").text,\n",
        "            jj.find_element(By.CSS_SELECTOR,\"span.date\").text])\n",
        "\n",
        "            try:\n",
        "                salary_list.append(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
        "\n",
        "            except NoSuchElementException:\n",
        "                try:\n",
        "                    salary_list.append(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
        "\n",
        "                except NoSuchElementException:\n",
        "                    salary_list.append(None)\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    return job_lst\n",
        "\n",
        "start = time.time()\n",
        "scraping = scraping_indeed(url,job,location)\n",
        "\n",
        "names = ['Job', 'Link', 'ID job', 'Company', 'Location', 'Date']\n",
        "scraping_df = pd.DataFrame(scraping, columns=names)\n",
        "scraping_df['Date'] = scraping_df['Date'].str.replace('Posted\\n', '')\n",
        "scraping_df.to_csv('scraping_indeed.csv', index=False)\n",
        "\n",
        "end = time.time()\n",
        "print('Task completed in: ',end - start,' seconds')\n",
        "\n",
        "scraping_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ewke-9bfqaRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze7YzRA0DELu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}